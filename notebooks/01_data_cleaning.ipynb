{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc790c5-b9fb-48c6-8461-b2d2712df0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Notebook - Phelps et al. 2016 Glass Dataset\n",
    "# Simple data cleaning procedures before analysis\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Starting data cleaning process...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44679914-6ff8-475f-8d07-ba83c84bb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Load Raw Data\n",
    "# ============================================================================\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = '/app/data/raw/phelps_et_al_2016.xlsx'\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_excel(file_path)\n",
    "    print(f\"✅ Data loaded successfully\")\n",
    "    print(f\"Original dataset shape: {df_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File not found. Check the file path.\")\n",
    "    print(\"Available files in data/raw/:\")\n",
    "    print(list(Path('/app/data/raw/').glob('*')))\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of raw data:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6a159-1ffa-45bd-a3f6-3aabda9f51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Initial Data Inspection\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INITIAL DATA INSPECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic info\n",
    "print(f\"Dataset shape: {df_raw.shape}\")\n",
    "print(f\"Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Column information\n",
    "print(f\"\\nColumn names and types:\")\n",
    "for i, (col, dtype) in enumerate(zip(df_raw.columns, df_raw.dtypes), 1):\n",
    "    print(f\"{i:2d}. {col:<25} ({dtype})\")\n",
    "\n",
    "# Check for duplicate column names\n",
    "duplicate_cols = df_raw.columns[df_raw.columns.duplicated()].tolist()\n",
    "if duplicate_cols:\n",
    "    print(f\"\\n⚠️ Duplicate column names found: {duplicate_cols}\")\n",
    "else:\n",
    "    print(f\"\\n✅ No duplicate column names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1734b7-d9d2-4e79-87e6-4daf3cde121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Column Name Standardization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN NAME STANDARDIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# Store original column names for reference\n",
    "original_columns = df_clean.columns.tolist()\n",
    "\n",
    "# Standardize column names: lowercase, remove spaces, replace special chars\n",
    "df_clean.columns = (df_clean.columns\n",
    "                   .str.lower()                    # Convert to lowercase\n",
    "                   .str.replace(' ', '_')          # Replace spaces with underscores\n",
    "                   .str.replace('-', '_')          # Replace hyphens with underscores\n",
    "                   .str.replace('(', '')           # Remove parentheses\n",
    "                   .str.replace(')', '')\n",
    "                   .str.replace('%', 'pct')        # Replace % with pct\n",
    "                   .str.replace('₂', '2')          # Replace subscript 2\n",
    "                   .str.replace('₃', '3')          # Replace subscript 3\n",
    "                   .str.strip('_'))                # Remove leading/trailing underscores\n",
    "\n",
    "# Show column name changes\n",
    "print(\"Column name changes:\")\n",
    "for old, new in zip(original_columns, df_clean.columns):\n",
    "    if old != new:\n",
    "        print(f\"  '{old}' → '{new}'\")\n",
    "    \n",
    "print(f\"\\nStandardized column names:\")\n",
    "for i, col in enumerate(df_clean.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf927f78-02cd-47a4-903b-e39645e89a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Missing Values Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = df_clean.isnull().sum()\n",
    "missing_percent = (missing_data / len(df_clean)) * 100\n",
    "\n",
    "# Create missing values summary\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percentage': missing_percent.values,\n",
    "    'Data_Type': df_clean.dtypes.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Show columns with missing values\n",
    "columns_with_missing = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "\n",
    "if len(columns_with_missing) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(columns_with_missing.to_string(index=False))\n",
    "    \n",
    "    # Visualize missing data pattern\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Missing values heatmap\n",
    "    plt.subplot(1, 2, 1)\n",
    "    missing_cols = columns_with_missing.head(20)['Column'].tolist()\n",
    "    if missing_cols:\n",
    "        sns.heatmap(df_clean[missing_cols].isnull(), \n",
    "                   cbar=True, yticklabels=False, cmap='viridis')\n",
    "        plt.title('Missing Values Pattern')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # Missing values bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_missing = columns_with_missing.head(10)\n",
    "    plt.barh(top_missing['Column'], top_missing['Missing_Percentage'])\n",
    "    plt.xlabel('Missing Percentage (%)')\n",
    "    plt.title('Top 10 Columns by Missing Data')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"✅ No missing values found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724885b8-89f7-4d36-a150-aa6e9651e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Duplicate Rows Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DUPLICATE ROWS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for complete duplicates\n",
    "total_duplicates = df_clean.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {total_duplicates}\")\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    print(\"\\nDuplicate rows found:\")\n",
    "    duplicate_rows = df_clean[df_clean.duplicated(keep=False)]\n",
    "    print(duplicate_rows)\n",
    "    \n",
    "    # Option to remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"✅ No duplicate rows found\")\n",
    "\n",
    "# Check for duplicates in key identifier columns\n",
    "id_columns = [col for col in df_clean.columns if 'id' in col.lower() or 'sample' in col.lower()]\n",
    "if id_columns:\n",
    "    print(f\"\\nChecking identifier columns: {id_columns}\")\n",
    "    for col in id_columns:\n",
    "        duplicates = df_clean[col].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"  {col}: {duplicates} duplicate values\")\n",
    "        else:\n",
    "            print(f\"  {col}: ✅ No duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a293e-511a-4e2c-bb0e-082209cd53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Data Types Analysis and Cleaning\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show current data types\n",
    "print(\"Current data types:\")\n",
    "print(df_clean.dtypes.value_counts())\n",
    "\n",
    "# Identify numeric columns that might be stored as text\n",
    "print(f\"\\nColumns analysis:\")\n",
    "for col in df_clean.columns:\n",
    "    dtype = df_clean[col].dtype\n",
    "    unique_count = df_clean[col].nunique()\n",
    "    sample_values = df_clean[col].dropna().head(3).tolist()\n",
    "    \n",
    "    print(f\"{col:<20} | {str(dtype):<10} | {unique_count:>3} unique | Sample: {sample_values}\")\n",
    "\n",
    "# Convert object columns to numeric where appropriate\n",
    "print(f\"\\nAttempting to convert object columns to numeric...\")\n",
    "\n",
    "numeric_conversions = 0\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == 'object':\n",
    "        # Try to convert to numeric\n",
    "        try:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            numeric_conversions += 1\n",
    "            print(f\"  ✅ Converted {col} to numeric\")\n",
    "        except:\n",
    "            print(f\"  ⚠️ Could not convert {col} to numeric (keeping as text)\")\n",
    "\n",
    "print(f\"Converted {numeric_conversions} columns to numeric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1e285-c712-4143-943e-7abc41b3a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Outlier Detection\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns for outlier analysis: {len(numeric_cols)}\")\n",
    "\n",
    "if numeric_cols:\n",
    "    # Calculate outliers using IQR method\n",
    "    outlier_summary = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].notna().sum() > 0:  # Only if column has data\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)][col]\n",
    "            outlier_count = len(outliers)\n",
    "            \n",
    "            outlier_summary.append({\n",
    "                'Column': col,\n",
    "                'Outlier_Count': outlier_count,\n",
    "                'Outlier_Percentage': (outlier_count / len(df_clean)) * 100,\n",
    "                'Min_Value': df_clean[col].min(),\n",
    "                'Max_Value': df_clean[col].max(),\n",
    "                'Lower_Bound': lower_bound,\n",
    "                'Upper_Bound': upper_bound\n",
    "            })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_Count', ascending=False)\n",
    "    \n",
    "    # Show columns with outliers\n",
    "    outliers_found = outlier_df[outlier_df['Outlier_Count'] > 0]\n",
    "    if len(outliers_found) > 0:\n",
    "        print(\"Columns with outliers:\")\n",
    "        print(outliers_found.to_string(index=False))\n",
    "    else:\n",
    "        print(\"✅ No outliers detected using IQR method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53b68a-8aac-4cbf-8cf1-70c80ec733c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Data Consistency Checks\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA CONSISTENCY CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for negative values in oxide/element columns (should be positive)\n",
    "oxide_element_cols = [col for col in numeric_cols if any(x in col.lower() for x in ['o', 'sio', 'cao', 'al', 'na', 'mg', 'fe', 'ti', 'k'])]\n",
    "\n",
    "negative_checks = []\n",
    "for col in oxide_element_cols:\n",
    "    negative_count = (df_clean[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        negative_checks.append({'Column': col, 'Negative_Values': negative_count})\n",
    "\n",
    "if negative_checks:\n",
    "    print(\"⚠️ Columns with negative values (may be data entry errors):\")\n",
    "    for check in negative_checks:\n",
    "        print(f\"  {check['Column']}: {check['Negative_Values']} negative values\")\n",
    "else:\n",
    "    print(\"✅ No negative values in chemical composition columns\")\n",
    "\n",
    "# Check for unrealistic percentages (>100% in oxide data)\n",
    "percentage_checks = []\n",
    "for col in oxide_element_cols:\n",
    "    if df_clean[col].notna().sum() > 0:\n",
    "        max_val = df_clean[col].max()\n",
    "        if max_val > 100:\n",
    "            percentage_checks.append({'Column': col, 'Max_Value': max_val})\n",
    "\n",
    "if percentage_checks:\n",
    "    print(\"⚠️ Columns with values >100% (check if these are percentages):\")\n",
    "    for check in percentage_checks:\n",
    "        print(f\"  {check['Column']}: max value = {check['Max_Value']:.2f}\")\n",
    "else:\n",
    "    print(\"✅ No unrealistic percentage values found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
